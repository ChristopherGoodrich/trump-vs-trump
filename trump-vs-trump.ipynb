{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Analysis via Latent Semantic Analysis\n",
    "\n",
    "0. load data (Trump's tweets)\n",
    "\n",
    "1. create term vectors\n",
    "\n",
    "2. calculate TF-IDF and perform SVD on it\n",
    "\n",
    "3. look at topics and look at tweets with terms of interest\n",
    "\n",
    "---\n",
    "\n",
    "**big question:** What was Trump tweeting about at different periods?\n",
    "\n",
    "**planned additions:** Perform sentiment analysis and see how that changes over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### load data and clean\n",
    "Trump tweets from http://www.trumptwitterarchive.com/archive\n",
    "\n",
    "looking at two time periods in this notebook\n",
    "\n",
    "* time period 1 - Trump campaign annoucement in 2015 to election day 2016\n",
    "* time period 2 - Start to end of Mueller investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11120</th>\n",
       "      <td>LIVE on #Periscope: Join me for a few minutes ...</td>\n",
       "      <td>2016-11-07 23:28:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11121</th>\n",
       "      <td>Hey Missouri let's defeat Crooked Hillary  @ko...</td>\n",
       "      <td>2016-11-07 22:21:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11122</th>\n",
       "      <td>'America must decide between failed policies o...</td>\n",
       "      <td>2016-11-07 21:37:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text                date\n",
       "11120  LIVE on #Periscope: Join me for a few minutes ... 2016-11-07 23:28:48\n",
       "11121  Hey Missouri let's defeat Crooked Hillary  @ko... 2016-11-07 22:21:53\n",
       "11122  'America must decide between failed policies o... 2016-11-07 21:37:25"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = load_data()\n",
    "c = clean_data(d)\n",
    "dt, drt = split_retweets(c) #split twts and re-twts\n",
    "\n",
    "# campaign annoucement to election day\n",
    "cond1 = ((dt['date'] >= pd.Timestamp('2015-07-16'))\n",
    "          & (dt['date'] <= pd.Timestamp('2016-11-08')))\n",
    "\n",
    "# start to end of Mueller investigation\n",
    "cond2 = ((dt['date'] >= pd.Timestamp('2017-05-17'))\n",
    "          & (dt['date'] <= pd.Timestamp('2019-03-22')))\n",
    "\n",
    "d1 = dt[cond1]\n",
    "d2 = dt[cond2]\n",
    "\n",
    "d1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### create term vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arg 2 - minmum number of terms in vec to use twt in analysis\n",
    "tvsdf1 = twtsdf_to_tvsdf(d1, min_vec_len=8)\n",
    "tvsdf2 = twtsdf_to_tvsdf(d2, min_vec_len=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>tvs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey Missouri let's defeat Crooked Hillary  @ko...</td>\n",
       "      <td>2016-11-07 22:21:53</td>\n",
       "      <td>[hey, missouri, let, defeat, crook, hillari, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'America must decide between failed policies o...</td>\n",
       "      <td>2016-11-07 21:37:25</td>\n",
       "      <td>[must, decid, fail, polici, fresh, perspect, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just landed in North Carolina- heading to the ...</td>\n",
       "      <td>2016-11-07 19:30:12</td>\n",
       "      <td>[land, north, carolina, head, js, dorton, aren...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                date  \\\n",
       "0  Hey Missouri let's defeat Crooked Hillary  @ko... 2016-11-07 22:21:53   \n",
       "1  'America must decide between failed policies o... 2016-11-07 21:37:25   \n",
       "2  Just landed in North Carolina- heading to the ... 2016-11-07 19:30:12   \n",
       "\n",
       "                                                 tvs  \n",
       "0  [hey, missouri, let, defeat, crook, hillari, k...  \n",
       "1  [must, decid, fail, polici, fresh, perspect, c...  \n",
       "2  [land, north, carolina, head, js, dorton, aren...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvsdf1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>tvs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Today we celebrate the lives and achievements ...</td>\n",
       "      <td>2019-03-21 21:12:05</td>\n",
       "      <td>[celebr, live, achiev, american, syndrom, alwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We are here today to take historic action to d...</td>\n",
       "      <td>2019-03-21 20:12:40</td>\n",
       "      <td>[take, histor, action, defend, american, stude...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After 52 years it is time for the United State...</td>\n",
       "      <td>2019-03-21 16:50:46</td>\n",
       "      <td>[year, unit, fulli, recogn, israel, sovereignt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                date  \\\n",
       "0  Today we celebrate the lives and achievements ... 2019-03-21 21:12:05   \n",
       "1  We are here today to take historic action to d... 2019-03-21 20:12:40   \n",
       "2  After 52 years it is time for the United State... 2019-03-21 16:50:46   \n",
       "\n",
       "                                                 tvs  \n",
       "0  [celebr, live, achiev, american, syndrom, alwa...  \n",
       "1  [take, histor, action, defend, american, stude...  \n",
       "2  [year, unit, fulli, recogn, israel, sovereignt...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvsdf2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### calculate TF-IDF and perform SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1000)\n",
      "(100, 1000)\n"
     ]
    }
   ],
   "source": [
    "# arg 2 - number of features\n",
    "# arg 3 - number of PCs\n",
    "svd1, v1 = tvs_to_svd(tvsdf1['tvs'], \n",
    "                      num_features=1000, \n",
    "                      num_comps=100)\n",
    "\n",
    "svd2, v2 = tvs_to_svd(tvsdf2['tvs'], \n",
    "                      num_features=1000, \n",
    "                      num_comps=100)\n",
    "\n",
    "print(svd1.components_.shape)\n",
    "print(svd2.components_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0: \n",
      "fals hillari clinton android poll crook say cruz said debat win bad watch total support job show run lead cant ted need jeb campaign fail beat last rubio know email \n",
      "\n",
      "Topic 1: \n",
      "hillari clinton crook email berni bad judgement sander rig isi said obama system run bill corrupt beat year total question fbi video want person husband decis scandal made lie dishonest \n",
      "\n",
      "Topic 2: \n",
      "cruz poll ted rubio carson iowa marco kasich jeb bush lead wow debat win lyin last senat show fail lightweight beat total number report campaign money said say candid john \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# arg 3 - number of topics\n",
    "# arg 4 - number of top words from PC to show\n",
    "print_top_topics(svd1, v1, num_topics=3, num_words=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0: \n",
      "border job work wall secur tax crime fake senat militari must need american cut strong total deal come trade nation year elect hous win dem done unit meet hard endors \n",
      "\n",
      "Topic 1: \n",
      "border wall secur crime strong militari need endors vet tax amend southern love nd immigr must drug cut vote senat full law governor congressman congress open build weak tough stop \n",
      "\n",
      "Topic 2: \n",
      "korea trade north china meet deal unit tariff kim tax billion honor nation american year forward talk un work xi cut jong south dollar world pay first farmer negoti come \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_topics(svd2, v2, num_topics=3, num_words=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### finding tweets with terms of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet date: 2019-02-21 20:10:38\n",
      "---\n",
      "terms: ['senat', 'john', 'cornyn', 'done', 'outstand', 'job', 'texa', 'strong', 'crime', 'border', 'second', 'amend', 'love', 'militari', 'vet', 'john', 'complet', 'total', 'endors']\n",
      "---\n",
      "tweet: Senator John Cornyn has done an outstanding job for the people of Texas. He is strong on Crime the Border the Second Amendment and loves our Military and Vets. John has my complete and total endorsement. MAKE AMERICA GREAT AGAIN!\n",
      "\n",
      "\n",
      "tweet date: 2019-02-15 03:16:23\n",
      "---\n",
      "terms: ['tri', 'use', 'th', 'amend', 'tri', 'circumv', 'elect', 'despic', 'act', 'unconstitut', 'power', 'grabbingwhich', 'happen', 'third', 'world', 'obey', 'law', 'attack', 'system', 'constitut', 'alan', 'dershowitz']\n",
      "---\n",
      "tweet: “Trying to use the 25th Amendment to try and circumvent the Election is a despicable act of unconstitutional power grabbing...which happens in third world countries. You have to obey the law. This is an attack on our system  Constitution.” Alan Dershowitz. @TuckerCarlson\n",
      "\n",
      "\n",
      "tweet date: 2018-11-04 01:39:58\n",
      "---\n",
      "terms: ['guy', 'team', 'player', 'repres', 'district', 'highest', 'level', 'danni', 'strong', 'militari', 'vet', 'second', 'amend', 'thing', 'strongli', 'stand', 'danni', 'strong', 'endors']\n",
      "---\n",
      "tweet: .@DannyTarkanian is a great guy and a team player. He will represent his District State and Country at the highest level. Danny is strong on Military our Vets Second Amendment and all of the things that we so strongly stand for. Vote for Danny - he has my Strong Endorsement!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_twts_with_terms(tvsdf2, terms=['amend'], n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "### packages\n",
    "#-------------------------------------------------------------------------------\n",
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import datetime\n",
    "\n",
    "# packages for text analysis\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# LSA via SVD\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "### load data (from http://www.trumptwitterarchive.com/archive)\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def load_data():\n",
    "    d = pd.read_csv('./data/tweets_all.csv')\n",
    "    d = d[['text', 'created_at']]\n",
    "    d.columns = ['text', 'date']\n",
    "    return d\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "### clean data \n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def clean_data(d):\n",
    "    c = d.copy()\n",
    "\n",
    "    ## cleaning from issues in raw data\n",
    "\n",
    "    # missing dates\n",
    "    c = c.loc[c['date'].notnull(), :]\n",
    "\n",
    "    # unreadable dates\n",
    "    c['date'] = pd.to_datetime(c['date'], errors='coerce')\n",
    "    c = c.loc[c['date'].notnull(), :]\n",
    "\n",
    "    # removing ampersand text\n",
    "    c['text'] = c['text'].str.replace('&amp;', '')\n",
    "    \n",
    "    return c\n",
    "\n",
    "# split tweets and retweets\n",
    "def split_retweets(d):\n",
    "    cond = d['text'].str.find('RT', 0, 2) != -1 #retweets\n",
    "    twts = c.loc[~cond, :].reset_index(drop=True)\n",
    "    rtwts = c.loc[cond, :].reset_index(drop=True)\n",
    "    return twts, rtwts\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "### create term vectors (including stop word removal and stemming)\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def twts_to_tvs(twts):\n",
    "    '''turns tweets into term vectors'''\n",
    "    tvs = twts.apply(twt_clean_split_to_tv)\n",
    "    tvs = tvs.apply(tv_remove_stopwords)\n",
    "    tvs = tvs.apply(tv_stem)\n",
    "    return tvs\n",
    "\n",
    "def twt_clean_split_to_tv(twt):\n",
    "    '''cleans characters and splits into term vector'''\n",
    "    twt = twt.lower() # lower case\n",
    "    twt = re.sub(r'http\\S+', '', twt) # remove URL\n",
    "    twt = re.sub('\\d+', '', twt) # remove digits\n",
    "    twt = re.sub(r'\\B#\\w*[a-zA-Z]+\\w*', '', twt) # remove hashtag\n",
    "    twt = re.sub('@[^\\s]+','', twt) # remove @username\n",
    "\n",
    "    # odd characters found not in string.punctuation\n",
    "    odd_chars = ('“', '”', '’', '‘')\n",
    "    chrs = string.punctuation.join(odd_chars)\n",
    "    twt = (re.compile('[%s]' % re.escape(chrs))\n",
    "             .sub('', twt))\n",
    "    \n",
    "    twt = nltk.word_tokenize(twt)\n",
    "    return twt\n",
    "\n",
    "def create_swords():\n",
    "    '''function for defining stop words to be used'''\n",
    "    \n",
    "    # these are largely chosen when they were found to \n",
    "    # obscure the meaning of a topic grouping in latent\n",
    "    # semantic analysis\n",
    "    r_names = ['donald', 'trump', 'fox']\n",
    "    r_politics = ['democrat', 'democrats', \n",
    "                  'republican', 'republicans',\n",
    "                  'maga', 'president', 'presidents', 'presidency',\n",
    "                  'us', 'state', 'states', 'country', 'countries',\n",
    "                  'vote', 'usa']\n",
    "    r_nonwords = ['pm', 'pme']\n",
    "    r_numwords = ['one', 'two', 'three']\n",
    "    r_days = ['monday', 'tuesday', 'wednesday', 'thursday',\n",
    "              'friday', 'saturday', 'sunday',\n",
    "              'morning', 'night', 'tonight', \n",
    "              'day', 'week', 'year',\n",
    "              'today']\n",
    "    r_other = ['make', 'america', 'great', 'again',\n",
    "               'thank', 'thanks', 'you', 'tonight', 'get', 'go',\n",
    "               'people', 'new', 'news', 'twitter', 'media',\n",
    "               'much', 'good', 'big', 'want', 'look', 'like',\n",
    "               'many', 'morning', 'tonight', 'night', 'time',\n",
    "               'never', 'would', 'back', 'go', 'even',\n",
    "               'one', 'going']\n",
    "    \n",
    "    rmv = (r_names + r_politics + r_nonwords + \n",
    "           r_numwords + r_days + r_other)\n",
    "    swords = nltk.corpus.stopwords.words('english') + rmv\n",
    "    \n",
    "    swords = [re.sub('[^A-Za-z0-9]+', '', s) \n",
    "              for s in swords] # remove punc\n",
    "    \n",
    "    return swords\n",
    "\n",
    "def tv_remove_stopwords(tv):\n",
    "    swords = create_swords()\n",
    "    newtv = []\n",
    "    for t in tv:\n",
    "        if t not in swords:\n",
    "            newtv.append(t)\n",
    "    return newtv\n",
    "\n",
    "def tv_stem(tv):\n",
    "    '''stem a term vector'''\n",
    "    \n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    for i in range(0, len(tv)):\n",
    "        tv[i] = stemmer.stem(tv[i])\n",
    "    return tv\n",
    "\n",
    "def twtsdf_to_tvsdf(twtsdf, min_vec_len):\n",
    "    '''adds column of term vectors to df with tweets'''\n",
    "    twtsdf = twtsdf.copy()\n",
    "    twtsdf['tvs'] = twts_to_tvs(twtsdf['text'])\n",
    "    twtsdf = twtsdf[twtsdf['tvs'].map(len) >= min_vec_len]\n",
    "    twtsdf = twtsdf.reset_index(drop=True)\n",
    "    return twtsdf\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "### create TF-IDF matrix and perform SVD on it (i.e. LSA)\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def tvs_to_svd(tvs, num_features, num_comps):\n",
    "    '''take term vectors (tvs) and perform svd on tf-idf'''\n",
    "    tvs = list(tvs.apply(lambda x: ' '.join(x)))\n",
    "    \n",
    "    v = TfidfVectorizer(max_features=num_features) # need to pass out to get\n",
    "    tfidf = v.fit_transform(tvs)\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=num_comps, \n",
    "                       algorithm='randomized', \n",
    "                       n_iter=100, random_state=123)\n",
    "    svd.fit(tfidf)    \n",
    "    return svd, v\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "### exploring results\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def print_top_topics(svd, v, num_topics, num_words):\n",
    "    '''prints top words from top topics from LSA'''\n",
    "    terms = v.get_feature_names()\n",
    "\n",
    "    for i, comp in enumerate(svd.components_[:num_topics]):\n",
    "        terms_comp = zip(terms, comp)\n",
    "        sorted_terms = sorted(terms_comp, key=lambda x: x[1], reverse=True)[:num_words]\n",
    "        print(\"\\nTopic \"+str(i)+\": \")\n",
    "        for t in sorted_terms:\n",
    "            print(t[0], end=' ')\n",
    "        print()\n",
    "    print()\n",
    "        \n",
    "def print_twts_with_terms(tvsdf, terms, n):\n",
    "    '''prints n tweets containg given a list of term vector terms'''\n",
    "    count = 0\n",
    "    for i in range(len(tvsdf)):\n",
    "        if count==n:\n",
    "            break\n",
    "        if bool(set(terms) & set(tvsdf['tvs'][i])):\n",
    "            print('tweet date: {}\\n---'.format(tvsdf['date'][i]))\n",
    "            print('terms: {}\\n---'.format(tvsdf['tvs'][i]))\n",
    "            print('tweet: {}\\n\\n'.format(tvsdf['text'][i]))\n",
    "            count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py37_trump-vs-trump",
   "language": "python",
   "name": "py37_trump-vs-trump"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
